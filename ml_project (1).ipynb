{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFYMbTk57SzK"
      },
      "source": [
        "## Attempt 1: Pre-processing the data (parsing dates, age info converted into days, encoding categorical features, handling missing values using median for numeric and most common for categorical). Ensemble method that combines RandomForest, XGBoost, and MLP. Hyperparameter tuning using CV. Evaluation metrics: accuracy and log loss.\n",
        "\n",
        "test score: .39\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diPsFaeg9Wb5",
        "outputId": "caa06809-2603-47cf-b43e-36bccf307bcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-33369bf525f4>:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
            "<ipython-input-22-33369bf525f4>:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
            "<ipython-input-22-33369bf525f4>:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the Combined VotingClassifier (RandomizedSearchCV)...\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "\n",
            "--- Validation Results ---\n",
            "Best Parameters: {'classifier__xgb__n_estimators': 150, 'classifier__xgb__max_depth': 4, 'classifier__xgb__learning_rate': 0.1, 'classifier__rf__n_estimators': 150, 'classifier__rf__max_depth': 12, 'classifier__mlp__max_iter': 300, 'classifier__mlp__hidden_layer_sizes': (100, 50), 'classifier__mlp__alpha': 0.0001, 'classifier__mlp__activation': 'relu'}\n",
            "Validation Accuracy: 0.6376\n",
            "Validation Log Loss: 0.8766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“„ Submission file saved as 'submission_optimized.csv'\n",
            "   Id     Outcome Type\n",
            "0   1  Return to Owner\n",
            "1   2         Transfer\n",
            "2   3  Return to Owner\n",
            "3   4         Adoption\n",
            "4   5         Transfer\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from xgboost import XGBClassifier\n",
        "import scipy.stats as st\n",
        "\n",
        "# === Preprocessing Function ===\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Function to convert age strings to days\n",
        "    def parse_age(age_str):\n",
        "        if pd.isna(age_str):\n",
        "            return np.nan\n",
        "        try:\n",
        "            num = int(''.join(filter(str.isdigit, str(age_str))))\n",
        "            unit = ''.join(filter(str.isalpha, str(age_str).lower()))\n",
        "            if 'year' in unit:\n",
        "                return num * 365\n",
        "            elif 'month' in unit:\n",
        "                return num * 30\n",
        "            elif 'week' in unit:\n",
        "                return num * 7\n",
        "            elif 'day' in unit:\n",
        "                return num\n",
        "            return np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    df['Age in Days'] = df.get('Age upon Intake', '').apply(parse_age)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract('([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories in these columns\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# === Preprocess Data ===\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' from training if missing in test set\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Extra safety: drop all-NaN columns\n",
        "X_train.dropna(axis=1, how='all', inplace=True)\n",
        "X_val.dropna(axis=1, how='all', inplace=True)\n",
        "X_test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# === Define Feature Lists for Preprocessing ===\n",
        "numeric_features = [\n",
        "    'Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "    'Intake Weekday', 'Intake Hour', 'Color Count'\n",
        "]\n",
        "categorical_features = [\n",
        "    'Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "    'Primary Color', 'Secondary Color'\n",
        "]\n",
        "if 'Length of Stay' in X_train.columns:\n",
        "    numeric_features.append('Length of Stay')\n",
        "\n",
        "numeric_features = [f for f in numeric_features if f in X_train.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X_train.columns]\n",
        "\n",
        "# === Build Preprocessing Pipeline ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# === Build Combined VotingClassifier Pipeline ===\n",
        "# This VotingClassifier combines three models: RandomForest, XGBoost, and MLP.\n",
        "voting_clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
        "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)),\n",
        "            ('mlp', MLPClassifier(random_state=42))\n",
        "        ],\n",
        "        voting='soft', n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# === Define a Reduced Hyperparameter Space for Faster Tuning ===\n",
        "param_grid = {\n",
        "    'classifier__rf__n_estimators': [100, 150],\n",
        "    'classifier__rf__max_depth': [12],\n",
        "    'classifier__xgb__n_estimators': [100, 150],\n",
        "    'classifier__xgb__max_depth': [4],\n",
        "    'classifier__xgb__learning_rate': [0.1],\n",
        "    'classifier__mlp__hidden_layer_sizes': [(100,), (100, 50)],\n",
        "    'classifier__mlp__alpha': [0.0001],\n",
        "    'classifier__mlp__activation': ['relu'],\n",
        "    'classifier__mlp__max_iter': [300]\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV for a faster search (20 iterations)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(voting_clf, param_distributions=param_grid,\n",
        "                                   n_iter=20, cv=3, scoring='accuracy',\n",
        "                                   n_jobs=-1, verbose=1, random_state=42)\n",
        "print(\"Tuning the Combined VotingClassifier (RandomizedSearchCV)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# === Evaluate on the Validation Set ===\n",
        "best_model = random_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# === Retrain the Best Model on Full Training Data ===\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# === Save Final Submission ===\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('submission_combined_optimized.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'submission_combined_optimized.csv'\")\n",
        "print(submission.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szcrhl7QSMj7"
      },
      "source": [
        "# Attempt 2\n",
        "Pre-processing the data (parsing dates, age info converted into days, encoding categorical features, handling missing values using median for numeric and most common for categorical).\n",
        "Ensmeble method with Stacking Classifier using Logistic Regression as base model, Random Forest Classifier, Gradient Boosting Classifier.\n",
        "Hyperparameter tuning using CV. Evaluation Metric: Accuracy.\n",
        "\n",
        "test score: .35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9oVFSYwSNCV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Data Preprocessing\n",
        "# ==============================================================================\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Convert \"Age upon Intake\" to days\n",
        "    def parse_age(age_str):\n",
        "        if pd.isna(age_str):\n",
        "            return np.nan\n",
        "        try:\n",
        "            num = int(''.join(filter(str.isdigit, str(age_str))))\n",
        "            unit = ''.join(filter(str.isalpha, str(age_str).lower()))\n",
        "            if 'year' in unit:\n",
        "                return num * 365\n",
        "            elif 'month' in unit:\n",
        "                return num * 30\n",
        "            elif 'week' in unit:\n",
        "                return num * 7\n",
        "            elif 'day' in unit:\n",
        "                return num\n",
        "            return np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    df['Age in Days'] = df.get('Age upon Intake', '').apply(parse_age)\n",
        "\n",
        "    # If Date of Birth exists, use it to fill missing Age\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Additional feature engineering\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract('([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories in Intake Condition and Animal Type\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if (is_train and 'Outcome Type' in df.columns) else None\n",
        "\n",
        "    # Feature set selection\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Load the Data\n",
        "# ==============================================================================\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Preprocess training and testing data\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' if missing in test set\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.reindex(columns=X.columns, fill_value=np.nan)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Encode the Target Variable\n",
        "# ==============================================================================\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Split Data into Training and Validation Sets\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Build Preprocessing Pipeline for Numeric and Categorical Variables\n",
        "# ==============================================================================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = [col for col in X_train.columns if X_train[col].dtype in [np.int64, np.float64]]\n",
        "categorical_features = [col for col in X_train.columns if col not in numeric_features]\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Build the Stacking Classifier Model\n",
        "# ==============================================================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Base models for StackingClassifier\n",
        "logreg = LogisticRegression(max_iter=500, random_state=42, solver='liblinear')\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Meta-model for stacking\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('logreg', logreg), ('rf', rf), ('gb', gb)],\n",
        "    final_estimator=meta_model,\n",
        "    cv=3\n",
        ")\n",
        "\n",
        "# Pipeline with preprocessing and stacking\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('clf', stacking_clf)\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Hyperparameter Tuning with RandomizedSearchCV\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'clf__logreg__C': [0.01, 0.1, 1],\n",
        "    'clf__rf__n_estimators': [100, 200],\n",
        "    'clf__rf__max_depth': [5, 10],\n",
        "    'clf__gb__n_estimators': [100, 200],\n",
        "    'clf__gb__learning_rate': [0.05, 0.1]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"Tuning the StackingClassifier ensemble...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", random_search.best_params_)\n",
        "print(f\"Best CV Balanced Accuracy: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. Evaluate on the Validation Set\n",
        "# ==============================================================================\n",
        "best_model = random_search.best_estimator_\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "val_bal_acc = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"\\nValidation Balanced Accuracy: {val_bal_acc:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. Retrain on Full Training Data and Predict Test Set Outcomes\n",
        "# ==============================================================================\n",
        "best_model.fit(X, y_encoded)\n",
        "test_preds = best_model.predict(X_test)\n",
        "test_preds_labels = le.inverse_transform(test_preds)\n",
        "\n",
        "# ==============================================================================\n",
        "# 10. Prepare Submission File\n",
        "# ==============================================================================\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': test_preds_labels\n",
        "})\n",
        "submission.to_csv('submission_stacking_ensemble.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'submission_stacking_ensemble.csv'\")\n",
        "print(submission.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xazfsW2U68IZ"
      },
      "source": [
        "# Attempt 3:\n",
        "Pre-processing the data (parsing dates, converting age information into days, encoding categorical features, and handling missing values using median for numeric and most common for categorical).\n",
        "Ensemble method using VotingClassifier with Random Forest Classifier, XGBoost, and MLPClassifier.\n",
        "Weighted voting applied to base models with adjusted weights based on validation results.\n",
        "Hyperparameter tuning using RandomizedSearchCV with 10 iterations and 3-fold cross-validation.\n",
        "Evaluation metric: Accuracy and Log Loss.\n",
        "\n",
        "third- 0.40232"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtTt4SFX69No",
        "outputId": "f157a371-488a-4377-a16e-fdd82f5c4e5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6121984a2a77>:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
            "<ipython-input-2-6121984a2a77>:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
            "<ipython-input-2-6121984a2a77>:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the Combined VotingClassifier with Weighted Voting (RandomizedSearchCV)...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "--- Validation Results ---\n",
            "Best Parameters: {'classifier__xgb__n_estimators': 200, 'classifier__xgb__max_depth': 6, 'classifier__xgb__learning_rate': 0.1, 'classifier__rf__n_estimators': 150, 'classifier__rf__max_depth': 15, 'classifier__mlp__max_iter': 300, 'classifier__mlp__hidden_layer_sizes': (100,), 'classifier__mlp__alpha': 0.0001, 'classifier__mlp__activation': 'tanh'}\n",
            "Validation Accuracy: 0.6453\n",
            "Validation Log Loss: 0.8568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“„ Submission file saved as 'third.csv'\n",
            "   Id     Outcome Type\n",
            "0   1  Return to Owner\n",
            "1   2         Transfer\n",
            "2   3  Return to Owner\n",
            "3   4         Adoption\n",
            "4   5       Euthanasia\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from xgboost import XGBClassifier\n",
        "from joblib import Memory\n",
        "\n",
        "# Set up caching for the preprocessing step\n",
        "memory = Memory(location='cachedir', verbose=0)\n",
        "\n",
        "@memory.cache\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns (vectorized)\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Vectorized age parsing using regex extraction\n",
        "    age_extracted = df['Age upon Intake'].astype(str).str.extract(r'(?P<num>\\d+)\\s*(?P<unit>\\w+)', expand=True)\n",
        "    def convert_age(row):\n",
        "        try:\n",
        "            num = int(row['num'])\n",
        "        except:\n",
        "            return np.nan\n",
        "        unit = row['unit'].lower() if pd.notna(row['unit']) else ''\n",
        "        if 'year' in unit:\n",
        "            return num * 365\n",
        "        elif 'month' in unit:\n",
        "            return num * 30\n",
        "        elif 'week' in unit:\n",
        "            return num * 7\n",
        "        elif 'day' in unit:\n",
        "            return num\n",
        "        else:\n",
        "            return np.nan\n",
        "    df['Age in Days'] = age_extracted.apply(convert_age, axis=1)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Feature engineering for categorical variables\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract(r'([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# === Preprocess (cached) ===\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' if not available in test set, then align columns\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Drop all-NaN columns (safety check)\n",
        "X_train.dropna(axis=1, how='all', inplace=True)\n",
        "X_val.dropna(axis=1, how='all', inplace=True)\n",
        "X_test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# === Define Feature Lists for Preprocessing ===\n",
        "numeric_features = [\n",
        "    'Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "    'Intake Weekday', 'Intake Hour', 'Color Count'\n",
        "]\n",
        "categorical_features = [\n",
        "    'Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "    'Primary Color', 'Secondary Color'\n",
        "]\n",
        "if 'Length of Stay' in X_train.columns:\n",
        "    numeric_features.append('Length of Stay')\n",
        "\n",
        "numeric_features = [f for f in numeric_features if f in X_train.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X_train.columns]\n",
        "\n",
        "# === Build Preprocessing Pipeline ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# === Build Combined VotingClassifier Pipeline with Weighted Voting ===\n",
        "voting_clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
        "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)),\n",
        "            ('mlp', MLPClassifier(random_state=42, early_stopping=True))\n",
        "        ],\n",
        "        voting='soft',\n",
        "        weights=[2, 3, 1],  # Adjust weights based on validation results\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# === Define an Expanded but Reasonable Hyperparameter Space ===\n",
        "param_grid = {\n",
        "    'classifier__rf__n_estimators': [100, 150, 200],\n",
        "    'classifier__rf__max_depth': [12, 15],\n",
        "    'classifier__xgb__n_estimators': [100, 150, 200],\n",
        "    'classifier__xgb__max_depth': [4, 6],\n",
        "    'classifier__xgb__learning_rate': [0.1, 0.01],\n",
        "    'classifier__mlp__hidden_layer_sizes': [(100,), (100, 50), (150, 100)],\n",
        "    'classifier__mlp__alpha': [0.0001, 0.001],\n",
        "    'classifier__mlp__activation': ['relu', 'tanh'],\n",
        "    'classifier__mlp__max_iter': [300]\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV with n_iter=10 and 3-fold CV for improved tuning\n",
        "random_search = RandomizedSearchCV(voting_clf, param_distributions=param_grid,\n",
        "                                   n_iter=10, cv=3, scoring='accuracy',\n",
        "                                   n_jobs=-1, verbose=1, random_state=42)\n",
        "print(\"Tuning the Combined VotingClassifier with Weighted Voting (RandomizedSearchCV)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# === Evaluate on Validation Set ===\n",
        "best_model = random_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# === Retrain the Best Model on Full Data and Predict on Test Set ===\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# === Save Final Submission ===\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('third.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'third.csv'\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abGjoH6oTmP2"
      },
      "source": [
        "# Attempt 4:\n",
        "Feature engineering with flags (e.g., IsMix, HasName, Neutered) and color features (PrimaryCol, SecondaryCol). Rare category replacement for Intake Condition and Animal Type. Early stopping for better model optimization based on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "CMNmPjQ1ANce",
        "outputId": "5d287dba-1c34-4e86-e58f-4f3ea7bb621d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-fc83c7e9daa3>:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df['Outcome Time'], errors='coerce')\n",
            "<ipython-input-6-fc83c7e9daa3>:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\tvalidation-mlogloss:1.52248\n",
            "[50]\tvalidation-mlogloss:0.81579\n",
            "[100]\tvalidation-mlogloss:0.79474\n",
            "[150]\tvalidation-mlogloss:0.78990\n",
            "[200]\tvalidation-mlogloss:0.78692\n",
            "[250]\tvalidation-mlogloss:0.78545\n",
            "[300]\tvalidation-mlogloss:0.78467\n",
            "[350]\tvalidation-mlogloss:0.78385\n",
            "[400]\tvalidation-mlogloss:0.78367\n",
            "[412]\tvalidation-mlogloss:0.78370\n",
            "Validation Accuracy: 0.6832493702770781\n",
            "Validation Log Loss: 0.7836408330144823\n",
            "   Id Outcome Type\n",
            "0   1     Adoption\n",
            "1   2     Transfer\n",
            "2   3     Adoption\n",
            "3   4     Adoption\n",
            "4   5   Euthanasia\n",
            "Validation Accuracy: 0.7263853904282116\n",
            "Validation Log Loss: 0.6654180973025768\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "`best_iteration` is only defined when early stopping is used.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fc83c7e9daa3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mdall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m  \u001b[0;31m# use best_iteration from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mbest_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2711\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2713\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   2714\u001b[0m             \u001b[0;34m\"`best_iteration` is only defined when early stopping is used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: `best_iteration` is only defined when early stopping is used."
          ]
        }
      ],
      "source": [
        "# If in Colab, install:\n",
        "# !pip install xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Feature engineering with guards\n",
        "def feature_engineer(df, is_train=True):\n",
        "    df = df.copy()\n",
        "    # Intake datetime\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    dt = df['Intake Time'].dt\n",
        "    df['Year'], df['Month'], df['Day'] = dt.year, dt.month, dt.day\n",
        "    df['Weekday'], df['Hour'] = dt.weekday, dt.hour\n",
        "\n",
        "    # StayDays\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df['Outcome Time'], errors='coerce')\n",
        "        df['StayDays'] = (df['Outcome Time'] - df['Intake Time'])\\\n",
        "                            .dt.total_seconds()/(24*3600)\n",
        "\n",
        "    # AgeDays\n",
        "    age_ex = df.get('Age upon Intake','').astype(str)\\\n",
        "                .str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "    def to_days(n,u):\n",
        "        try: n = int(n)\n",
        "        except: return np.nan\n",
        "        u = (u or '').lower()\n",
        "        if 'year' in u:  return n*365\n",
        "        if 'month' in u: return n*30\n",
        "        if 'week' in u:  return n*7\n",
        "        if 'day' in u:   return n\n",
        "        return np.nan\n",
        "    df['AgeDays'] = age_ex.apply(lambda r: to_days(r[0], r[1]), axis=1)\n",
        "\n",
        "    # Flags\n",
        "    df['IsMix'] = df.get('Breed','')\\\n",
        "                     .str.contains('Mix', na=False).astype(int)\n",
        "    sx = df.get('Sex upon Intake','')\n",
        "    df['SexOnly']  = sx.str.extract(r'([A-Za-z]+)', expand=False).fillna('Unknown')\n",
        "    df['Neutered']= sx.str.contains('Neutered|Spayed', na=False).astype(int)\n",
        "    df['HasName']= df['Name'].notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['InAustin']= df.get('Found Location','')\\\n",
        "                       .str.contains('Austin', na=False).astype(int)\n",
        "\n",
        "    # Color features\n",
        "    col = df.get('Color','').fillna('')\n",
        "    df['NumCols']      = col.str.count('/')+1\n",
        "    df['PrimaryCol']   = col.str.split('/').str[0].replace('', 'None')\n",
        "    df['SecondaryCol'] = col.str.split('/').str[1].fillna('None')\n",
        "\n",
        "    # Rareâ†’Other\n",
        "    for c in ['Intake Condition','Animal Type']:\n",
        "        if c in df.columns:\n",
        "            vc = df[c].value_counts()\n",
        "            rare = vc[vc<5].index\n",
        "            df[c] = df[c].replace(rare,'Other')\n",
        "\n",
        "    # Target\n",
        "    y = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    # Keep list\n",
        "    keep = [\n",
        "        'Intake Type','Intake Condition','Animal Type','SexOnly',\n",
        "        'AgeDays','IsMix','HasName','InAustin',\n",
        "        'Year','Month','Day','Weekday','Hour',\n",
        "        'NumCols','PrimaryCol','SecondaryCol'\n",
        "    ]\n",
        "    if is_train and 'StayDays' in df.columns:\n",
        "        keep.append('StayDays')\n",
        "\n",
        "    return (df[keep], y) if is_train else df[keep]\n",
        "\n",
        "# 2. Load & engineer\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "\n",
        "X, y   = feature_engineer(train, is_train=True)\n",
        "X_test = feature_engineer(test,  is_train=False)\n",
        "\n",
        "# 3. Align columns\n",
        "if 'StayDays' in X.columns and 'StayDays' not in X_test.columns:\n",
        "    X = X.drop('StayDays', axis=1)\n",
        "X_test = X_test[X.columns]\n",
        "\n",
        "# 4. Frequencyâ€encode categoricals\n",
        "cat_cols = ['Intake Type','Intake Condition','Animal Type',\n",
        "            'SexOnly','PrimaryCol','SecondaryCol']\n",
        "for c in cat_cols:\n",
        "    freq = X[c].value_counts(normalize=True)\n",
        "    X[c+'_freq']      = X[c].map(freq)\n",
        "    X_test[c+'_freq'] = X_test[c].map(freq).fillna(0.0)\n",
        "\n",
        "# 5. Numeric features\n",
        "num_feats = ['AgeDays','IsMix','HasName','InAustin',\n",
        "             'Year','Month','Day','Weekday','Hour','NumCols'] \\\n",
        "            + [c+'_freq' for c in cat_cols]\n",
        "\n",
        "# 6. Train/val split & label encode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# drop classes <2 samples\n",
        "vc   = pd.Series(y_enc).value_counts()\n",
        "mask = ~pd.Series(y_enc).isin(vc[vc<2].index)\n",
        "X, y_enc = X[mask], y_enc[mask]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X[num_feats], y_enc,\n",
        "    test_size=0.2, stratify=y_enc, random_state=42\n",
        ")\n",
        "\n",
        "# fill NaNs\n",
        "X_train.fillna(-1, inplace=True)\n",
        "X_val.fillna(-1, inplace=True)\n",
        "X_test.fillna(-1, inplace=True)\n",
        "\n",
        "# 7. Train via xgb.train for early stopping\n",
        "# 7. Train via xgb.train for early stopping\n",
        "import xgboost as xgb\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval   = xgb.DMatrix(X_val,   label=y_val)\n",
        "dall   = xgb.DMatrix(pd.concat([X_train, X_val]), label=np.concatenate([y_train, y_val]))\n",
        "dtest  = xgb.DMatrix(X_test[num_feats])   # <- use only numeric columns here\n",
        "\n",
        "num_class = len(np.unique(y_enc))\n",
        "params = {\n",
        "    'objective':        'multi:softprob',\n",
        "    'num_class':        num_class,\n",
        "    'learning_rate':    0.1,\n",
        "    'max_depth':        6,\n",
        "    'subsample':        0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'eval_metric':      'mlogloss',\n",
        "    'seed':             42\n",
        "}\n",
        "\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=500,\n",
        "    evals=[(dval, 'validation')],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=50\n",
        ")\n",
        "\n",
        "# 8. Validate\n",
        "val_pred   = bst.predict(dval)\n",
        "val_labels = np.argmax(val_pred, axis=1)\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, val_labels))\n",
        "print(\"Validation Log Loss:\", log_loss(y_val, val_pred))\n",
        "\n",
        "# 9. Retrain on all data\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dall,\n",
        "    num_boost_round=bst.best_iteration\n",
        ")\n",
        "\n",
        "# 10. Predict & submit\n",
        "test_prob = bst.predict(dtest)\n",
        "test_pred = np.argmax(test_prob, axis=1)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'Id':            test['Id'],\n",
        "    'Outcome Type':  le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('submission_xgb_train.csv', index=False)\n",
        "print(submission.head())\n",
        "\n",
        "\n",
        "# 8. Validate\n",
        "val_pred  = bst.predict(dval)\n",
        "val_labels= np.argmax(val_pred, axis=1)\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, val_labels))\n",
        "print(\"Validation Log Loss:\", log_loss(y_val, val_pred))\n",
        "\n",
        "# 9. Retrain on all data\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dall,\n",
        "    num_boost_round=bst.best_iteration  # use best_iteration from above\n",
        ")\n",
        "\n",
        "# 10. Predict & submit\n",
        "test_prob = bst.predict(dtest)\n",
        "test_pred = np.argmax(test_prob, axis=1)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'Id':            test['Id'],\n",
        "    'Outcome Type':  le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('submission_xgb_train.csv', index=False)\n",
        "print(submission.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVrT8aDCR--b",
        "outputId": "21b335a8-87ba-4331-d4b3-c7f6b7184fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn xgboost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIn3UU52RTiN"
      },
      "source": [
        "# 5th Attempt:\n",
        "Tried to use OneVsRestClassifier with a Logistic Regression classifier. Hyperparameter tuning using GridSearchCV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3xKSOZvjRVsO",
        "outputId": "6882cb76-c334-4617-ed7d-4d770104dfe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the One-vs-Rest Classifier (Logistic Regression)...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py\", line 376, in fit\n    self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py\", line 96, in _fit_binary\n    estimator.fit(X, y, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1222, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Stray'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2f9e8bf6ac3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tuning the One-vs-Rest Classifier (Logistic Regression)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    999\u001b[0m                     )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py\", line 376, in fit\n    self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py\", line 96, in _fit_binary\n    estimator.fit(X, y, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1222, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Stray'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Preprocessing Function\n",
        "# ---------------------------\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Parse Age upon Intake using regex extraction (e.g. \"2 years\")\n",
        "    age_extracted = df['Age upon Intake'].astype(str).str.extract(r'(?P<num>\\d+)\\s*(?P<unit>\\w+)', expand=True)\n",
        "    def convert_age(row):\n",
        "        try:\n",
        "            num = int(row['num'])\n",
        "        except:\n",
        "            return np.nan\n",
        "        unit = row['unit'].lower() if pd.notna(row['unit']) else ''\n",
        "        if 'year' in unit:\n",
        "            return num * 365\n",
        "        elif 'month' in unit:\n",
        "            return num * 30\n",
        "        elif 'week' in unit:\n",
        "            return num * 7\n",
        "        elif 'day' in unit:\n",
        "            return num\n",
        "        else:\n",
        "            return np.nan\n",
        "    df['Age in Days'] = age_extracted.apply(convert_age, axis=1)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Feature engineering for categorical/text features\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract(r'([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories in these columns\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load Data and Preprocess\n",
        "# ---------------------------\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# If \"Length of Stay\" exists in training but not in test, drop it and align columns\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Build Preprocessing Pipeline (One-Hot Encoding for Categoricals)\n",
        "# ---------------------------\n",
        "numeric_features = ['Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "                    'Intake Weekday', 'Intake Hour', 'Color Count']\n",
        "categorical_features = ['Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "                        'Primary Color', 'Secondary Color']\n",
        "if 'Length of Stay' in X_train.columns:\n",
        "    numeric_features.append('Length of Stay')\n",
        "numeric_features = [f for f in numeric_features if f in X_train.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X_train.columns]\n",
        "\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, numeric_features),\n",
        "    ('cat', categorical_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Build One-vs-Rest Classifier with Logistic Regression\n",
        "# ---------------------------\n",
        "ovr_model = OneVsRestClassifier(LogisticRegression(max_iter=500, random_state=42, solver='liblinear'))\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Hyperparameter Tuning using GridSearchCV\n",
        "# ---------------------------\n",
        "param_grid = {\n",
        "    'estimator__C': [0.01, 0.1, 1, 10],\n",
        "    'estimator__penalty': ['l2'],  # We can also tune 'l1' if using other solvers like 'saga'\n",
        "    'estimator__solver': ['liblinear'],\n",
        "    'estimator__max_iter': [100, 500, 1000]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(ovr_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"Tuning the One-vs-Rest Classifier (Logistic Regression)...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Evaluate on the Validation Set\n",
        "# ---------------------------\n",
        "best_model = grid_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Retrain Best Model on Full Data and Predict on Test Set\n",
        "# ---------------------------\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# ---------------------------\n",
        "# 8. Save Final Submission\n",
        "# ---------------------------\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('submission_ovr_lr_model.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'submission_ovr_lr_model.csv'\")\n",
        "print(submission.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKuAgSjVVu-V"
      },
      "source": [
        "# Attempt 6: Stacking Classifier ensemble with Logistic Regression, Random Forest, and Gradient Boosting as base models and Logistic Regression as the meta-model. Uses F-1 as evaluation metric to account for class imbalance.\n",
        "\n",
        "Test results: .37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I96duOpnVvJJ",
        "outputId": "f5710476-4901-4252-8797-1b398a51174c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-5022f0c1895c>:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
            "<ipython-input-1-5022f0c1895c>:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
            "<ipython-input-1-5022f0c1895c>:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the StackingClassifier ensemble...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss, make_scorer, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Data Preprocessing\n",
        "# ==============================================================================\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Convert \"Age upon Intake\" to days\n",
        "    def parse_age(age_str):\n",
        "        if pd.isna(age_str):\n",
        "            return np.nan\n",
        "        try:\n",
        "            num = int(''.join(filter(str.isdigit, str(age_str))))\n",
        "            unit = ''.join(filter(str.isalpha, str(age_str).lower()))\n",
        "            if 'year' in unit:\n",
        "                return num * 365\n",
        "            elif 'month' in unit:\n",
        "                return num * 30\n",
        "            elif 'week' in unit:\n",
        "                return num * 7\n",
        "            elif 'day' in unit:\n",
        "                return num\n",
        "            return np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    df['Age in Days'] = df.get('Age upon Intake', '').apply(parse_age)\n",
        "\n",
        "    # If Date of Birth exists, use it to fill missing Age\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Additional feature engineering\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract('([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories in Intake Condition and Animal Type\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if (is_train and 'Outcome Type' in df.columns) else None\n",
        "\n",
        "    # Feature set selection\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Load the Data\n",
        "# ==============================================================================\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Preprocess training and testing data\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' if missing in test set\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.reindex(columns=X.columns, fill_value=np.nan)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Encode the Target Variable\n",
        "# ==============================================================================\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Split Data into Training and Validation Sets\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Build Preprocessing Pipeline for Numeric and Categorical Variables\n",
        "# ==============================================================================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = [col for col in X_train.columns if X_train[col].dtype in [np.int64, np.float64]]\n",
        "categorical_features = [col for col in X_train.columns if col not in numeric_features]\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. Build the Stacking Classifier Model\n",
        "# ==============================================================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Base models for StackingClassifier\n",
        "logreg = LogisticRegression(max_iter=500, random_state=42, solver='liblinear')\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Meta-model for stacking\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('logreg', logreg), ('rf', rf), ('gb', gb)],\n",
        "    final_estimator=meta_model,\n",
        "    cv=3\n",
        ")\n",
        "\n",
        "# Pipeline with preprocessing and stacking\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('clf', stacking_clf)\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Hyperparameter Tuning with RandomizedSearchCV using F1 Score\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'clf__logreg__C': [0.01, 0.1, 1],\n",
        "    'clf__rf__n_estimators': [100, 200],\n",
        "    'clf__rf__max_depth': [5, 10],\n",
        "    'clf__gb__n_estimators': [100, 200],\n",
        "    'clf__gb__learning_rate': [0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Use F1 score as the scoring metric\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=1, scoring=f1_scorer)\n",
        "\n",
        "print(\"Tuning the StackingClassifier ensemble...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", random_search.best_params_)\n",
        "print(f\"Best CV F1-Score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. Evaluate on the Validation Set\n",
        "# ==============================================================================\n",
        "best_model = random_search.best_estimator_\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "val_f1_score = f1_score(y_val, y_val_pred, average='weighted')\n",
        "print(f\"\\nValidation F1-Score: {val_f1_score:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. Retrain on Full Training Data and Predict Test Set Outcomes\n",
        "# ==============================================================================\n",
        "best_model.fit(X, y_encoded)\n",
        "test_preds = best_model.predict(X_test)\n",
        "test_preds_labels = le.inverse_transform(test_preds)\n",
        "\n",
        "# ==============================================================================\n",
        "# 10. Prepare Submission File\n",
        "# ==============================================================================\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': test_preds_labels\n",
        "})\n",
        "submission.to_csv('submission_stacking_ensemble_f1.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'submission_stacking_ensemble_f1.csv'\")\n",
        "print(submission.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
