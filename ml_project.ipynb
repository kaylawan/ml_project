{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFYMbTk57SzK"
      },
      "source": [
        "0.39467"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diPsFaeg9Wb5",
        "outputId": "caa06809-2603-47cf-b43e-36bccf307bcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-33369bf525f4>:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
            "<ipython-input-22-33369bf525f4>:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
            "<ipython-input-22-33369bf525f4>:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the Combined VotingClassifier (RandomizedSearchCV)...\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "\n",
            "--- Validation Results ---\n",
            "Best Parameters: {'classifier__xgb__n_estimators': 150, 'classifier__xgb__max_depth': 4, 'classifier__xgb__learning_rate': 0.1, 'classifier__rf__n_estimators': 150, 'classifier__rf__max_depth': 12, 'classifier__mlp__max_iter': 300, 'classifier__mlp__hidden_layer_sizes': (100, 50), 'classifier__mlp__alpha': 0.0001, 'classifier__mlp__activation': 'relu'}\n",
            "Validation Accuracy: 0.6376\n",
            "Validation Log Loss: 0.8766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“„ Submission file saved as 'submission_optimized.csv'\n",
            "   Id     Outcome Type\n",
            "0   1  Return to Owner\n",
            "1   2         Transfer\n",
            "2   3  Return to Owner\n",
            "3   4         Adoption\n",
            "4   5         Transfer\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from xgboost import XGBClassifier\n",
        "import scipy.stats as st\n",
        "\n",
        "# === Preprocessing Function ===\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Function to convert age strings to days\n",
        "    def parse_age(age_str):\n",
        "        if pd.isna(age_str):\n",
        "            return np.nan\n",
        "        try:\n",
        "            num = int(''.join(filter(str.isdigit, str(age_str))))\n",
        "            unit = ''.join(filter(str.isalpha, str(age_str).lower()))\n",
        "            if 'year' in unit:\n",
        "                return num * 365\n",
        "            elif 'month' in unit:\n",
        "                return num * 30\n",
        "            elif 'week' in unit:\n",
        "                return num * 7\n",
        "            elif 'day' in unit:\n",
        "                return num\n",
        "            return np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    df['Age in Days'] = df.get('Age upon Intake', '').apply(parse_age)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract('([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories in these columns\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# === Preprocess Data ===\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' from training if missing in test set\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Extra safety: drop all-NaN columns\n",
        "X_train.dropna(axis=1, how='all', inplace=True)\n",
        "X_val.dropna(axis=1, how='all', inplace=True)\n",
        "X_test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# === Define Feature Lists for Preprocessing ===\n",
        "numeric_features = [\n",
        "    'Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "    'Intake Weekday', 'Intake Hour', 'Color Count'\n",
        "]\n",
        "categorical_features = [\n",
        "    'Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "    'Primary Color', 'Secondary Color'\n",
        "]\n",
        "if 'Length of Stay' in X_train.columns:\n",
        "    numeric_features.append('Length of Stay')\n",
        "\n",
        "numeric_features = [f for f in numeric_features if f in X_train.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X_train.columns]\n",
        "\n",
        "# === Build Preprocessing Pipeline ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# === Build Combined VotingClassifier Pipeline ===\n",
        "# This VotingClassifier combines three models: RandomForest, XGBoost, and MLP.\n",
        "voting_clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
        "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)),\n",
        "            ('mlp', MLPClassifier(random_state=42))\n",
        "        ],\n",
        "        voting='soft', n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# === Define a Reduced Hyperparameter Space for Faster Tuning ===\n",
        "param_grid = {\n",
        "    'classifier__rf__n_estimators': [100, 150],\n",
        "    'classifier__rf__max_depth': [12],\n",
        "    'classifier__xgb__n_estimators': [100, 150],\n",
        "    'classifier__xgb__max_depth': [4],\n",
        "    'classifier__xgb__learning_rate': [0.1],\n",
        "    'classifier__mlp__hidden_layer_sizes': [(100,), (100, 50)],\n",
        "    'classifier__mlp__alpha': [0.0001],\n",
        "    'classifier__mlp__activation': ['relu'],\n",
        "    'classifier__mlp__max_iter': [300]\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV for a faster search (20 iterations)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(voting_clf, param_distributions=param_grid,\n",
        "                                   n_iter=20, cv=3, scoring='accuracy',\n",
        "                                   n_jobs=-1, verbose=1, random_state=42)\n",
        "print(\"Tuning the Combined VotingClassifier (RandomizedSearchCV)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# === Evaluate on the Validation Set ===\n",
        "best_model = random_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# === Retrain the Best Model on Full Training Data ===\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# === Save Final Submission ===\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('submission_combined_optimized.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'submission_combined_optimized.csv'\")\n",
        "print(submission.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xazfsW2U68IZ"
      },
      "source": [
        "third- 0.40232"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtTt4SFX69No",
        "outputId": "f157a371-488a-4377-a16e-fdd82f5c4e5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6121984a2a77>:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
            "<ipython-input-2-6121984a2a77>:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
            "<ipython-input-2-6121984a2a77>:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning the Combined VotingClassifier with Weighted Voting (RandomizedSearchCV)...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "--- Validation Results ---\n",
            "Best Parameters: {'classifier__xgb__n_estimators': 200, 'classifier__xgb__max_depth': 6, 'classifier__xgb__learning_rate': 0.1, 'classifier__rf__n_estimators': 150, 'classifier__rf__max_depth': 15, 'classifier__mlp__max_iter': 300, 'classifier__mlp__hidden_layer_sizes': (100,), 'classifier__mlp__alpha': 0.0001, 'classifier__mlp__activation': 'tanh'}\n",
            "Validation Accuracy: 0.6453\n",
            "Validation Log Loss: 0.8568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“„ Submission file saved as 'third.csv'\n",
            "   Id     Outcome Type\n",
            "0   1  Return to Owner\n",
            "1   2         Transfer\n",
            "2   3  Return to Owner\n",
            "3   4         Adoption\n",
            "4   5       Euthanasia\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from xgboost import XGBClassifier\n",
        "from joblib import Memory\n",
        "import re\n",
        "\n",
        "# Set up caching for the preprocessing step\n",
        "memory = Memory(location='cachedir', verbose=0)\n",
        "\n",
        "@memory.cache\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns (vectorized)\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Vectorized age parsing using regex extraction\n",
        "    age_extracted = df['Age upon Intake'].astype(str).str.extract(r'(?P<num>\\d+)\\s*(?P<unit>\\w+)', expand=True)\n",
        "    def convert_age(row):\n",
        "        try:\n",
        "            num = int(row['num'])\n",
        "        except:\n",
        "            return np.nan\n",
        "        unit = row['unit'].lower() if pd.notna(row['unit']) else ''\n",
        "        if 'year' in unit:\n",
        "            return num * 365\n",
        "        elif 'month' in unit:\n",
        "            return num * 30\n",
        "        elif 'week' in unit:\n",
        "            return num * 7\n",
        "        elif 'day' in unit:\n",
        "            return num\n",
        "        else:\n",
        "            return np.nan\n",
        "    df['Age in Days'] = age_extracted.apply(convert_age, axis=1)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Feature engineering for categorical variables\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract(r'([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    # Replace rare categories\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# === Preprocess (cached) ===\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Remove 'Length of Stay' if not available in test set, then align columns\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Drop all-NaN columns (safety check)\n",
        "X_train.dropna(axis=1, how='all', inplace=True)\n",
        "X_val.dropna(axis=1, how='all', inplace=True)\n",
        "X_test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# === Define Feature Lists for Preprocessing ===\n",
        "numeric_features = [\n",
        "    'Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "    'Intake Weekday', 'Intake Hour', 'Color Count'\n",
        "]\n",
        "categorical_features = [\n",
        "    'Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "    'Primary Color', 'Secondary Color'\n",
        "]\n",
        "if 'Length of Stay' in X_train.columns:\n",
        "    numeric_features.append('Length of Stay')\n",
        "\n",
        "numeric_features = [f for f in numeric_features if f in X_train.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X_train.columns]\n",
        "\n",
        "# === Build Preprocessing Pipeline ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# === Build Combined VotingClassifier Pipeline with Weighted Voting ===\n",
        "voting_clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
        "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)),\n",
        "            ('mlp', MLPClassifier(random_state=42, early_stopping=True))\n",
        "        ],\n",
        "        voting='soft',\n",
        "        weights=[2, 3, 1],  # Adjust weights based on validation results\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# === Define an Expanded but Reasonable Hyperparameter Space ===\n",
        "param_grid = {\n",
        "    'classifier__rf__n_estimators': [100, 150, 200],\n",
        "    'classifier__rf__max_depth': [12, 15],\n",
        "    'classifier__xgb__n_estimators': [100, 150, 200],\n",
        "    'classifier__xgb__max_depth': [4, 6],\n",
        "    'classifier__xgb__learning_rate': [0.1, 0.01],\n",
        "    'classifier__mlp__hidden_layer_sizes': [(100,), (100, 50), (150, 100)],\n",
        "    'classifier__mlp__alpha': [0.0001, 0.001],\n",
        "    'classifier__mlp__activation': ['relu', 'tanh'],\n",
        "    'classifier__mlp__max_iter': [300]\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV with n_iter=10 and 3-fold CV for improved tuning\n",
        "random_search = RandomizedSearchCV(voting_clf, param_distributions=param_grid,\n",
        "                                   n_iter=10, cv=3, scoring='accuracy',\n",
        "                                   n_jobs=-1, verbose=1, random_state=42)\n",
        "print(\"Tuning the Combined VotingClassifier with Weighted Voting (RandomizedSearchCV)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# === Evaluate on Validation Set ===\n",
        "best_model = random_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# === Retrain the Best Model on Full Data and Predict on Test Set ===\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# === Save Final Submission ===\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('third.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as 'third.csv'\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIn3UU52RTiN"
      },
      "source": [
        "4th-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVrT8aDCR--b",
        "outputId": "21b335a8-87ba-4331-d4b3-c7f6b7184fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3xKSOZvjRVsO",
        "outputId": "ddbcd4ee-7914-49e1-dbb0-39756f15eb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning the Advanced Pipeline (PCA + K-Means + SMOTE + Stacking)...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-1-d4f8d8ab32c7>\", line 254, in transform\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\", line 356, in hstack\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d4f8d8ab32c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m                                    n_jobs=-1, verbose=1, random_state=42)\n\u001b[1;32m    308\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tuning the Advanced Pipeline (PCA + K-Means + SMOTE + Stacking)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    999\u001b[0m                     )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-1-d4f8d8ab32c7>\", line 254, in transform\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\", line 356, in hstack\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Optionally suppress date parsing warnings (informational only)\n",
        "warnings.filterwarnings(\"ignore\", message=\"Could not infer format\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. Import Required Modules from scikit-learn, imblearn, etc.\n",
        "# ------------------------------------------------\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from joblib import Memory\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Define a Standard Preprocessor for Numeric and Categorical Features\n",
        "# ------------------------------------------------\n",
        "# Specify your numeric and categorical feature names (we assume these features are common in your dataset)\n",
        "numeric_features = ['Age in Days', 'Intake Year', 'Intake Month', 'Intake Day',\n",
        "                    'Intake Weekday', 'Intake Hour', 'Color Count']\n",
        "categorical_features = ['Intake Type', 'Intake Condition', 'Animal Type', 'Sex',\n",
        "                        'Primary Color', 'Secondary Color']\n",
        "\n",
        "# (Optionally include 'Length of Stay' if present)\n",
        "def get_feature_lists(X):\n",
        "    num_feats = list(numeric_features)\n",
        "    cat_feats = list(categorical_features)\n",
        "    if 'Length of Stay' in X.columns:\n",
        "        num_feats.append('Length of Stay')\n",
        "    num_feats = [f for f in num_feats if f in X.columns]\n",
        "    cat_feats = [f for f in cat_feats if f in X.columns]\n",
        "    return num_feats, cat_feats\n",
        "\n",
        "# Build pipelines for numeric and categorical features\n",
        "def build_preprocessor(X):\n",
        "    num_feats, cat_feats = get_feature_lists(X)\n",
        "    numeric_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "    preproc = ColumnTransformer([\n",
        "        ('num', numeric_pipeline, num_feats),\n",
        "        ('cat', categorical_pipeline, cat_feats)\n",
        "    ])\n",
        "    return preproc\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Define a Custom AdvancedPreprocessor Transformer\n",
        "# ------------------------------------------------\n",
        "class AdvancedPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This transformer applies a preprocessor (one-hot encoding for categoricals\n",
        "    and scaling for numerics), followed by PCA and then fits KMeans clustering.\n",
        "    The one-hot encoded cluster labels are appended to the original preprocessed features.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components=50, n_clusters=5):\n",
        "        self.n_components = n_components\n",
        "        self.n_clusters = n_clusters\n",
        "        self.preprocessor = None  # to be set in fit\n",
        "        self.pca = PCA(n_components=self.n_components, random_state=42)\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.preprocessor = build_preprocessor(pd.DataFrame(X))  # build preprocessor based on training columns\n",
        "        X_pre = self.preprocessor.fit_transform(X)\n",
        "        self.pca.fit(X_pre)\n",
        "        X_pca = self.pca.transform(X_pre)\n",
        "        self.kmeans.fit(X_pca)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_pre = self.preprocessor.transform(X)\n",
        "        X_pca = self.pca.transform(X_pre)\n",
        "        clusters = self.kmeans.predict(X_pca)\n",
        "        # One-hot encode cluster labels using pandas.get_dummies\n",
        "        cluster_dummies = pd.get_dummies(clusters, prefix=\"cluster\").values\n",
        "        return np.hstack([X_pre, cluster_dummies])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Preprocessing Function for Data Cleaning (with caching)\n",
        "# ------------------------------------------------\n",
        "memory = Memory(location='cachedir', verbose=0)\n",
        "\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse datetime columns\n",
        "    df['Intake Time'] = pd.to_datetime(df.get('Intake Time'), errors='coerce')\n",
        "    df['Intake Year'] = df['Intake Time'].dt.year\n",
        "    df['Intake Month'] = df['Intake Time'].dt.month\n",
        "    df['Intake Day'] = df['Intake Time'].dt.day\n",
        "    df['Intake Weekday'] = df['Intake Time'].dt.weekday\n",
        "    df['Intake Hour'] = df['Intake Time'].dt.hour\n",
        "\n",
        "    if is_train and 'Outcome Time' in df.columns:\n",
        "        df['Outcome Time'] = pd.to_datetime(df.get('Outcome Time'), errors='coerce')\n",
        "        df['Length of Stay'] = (df['Outcome Time'] - df['Intake Time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "    # Parse Age upon Intake using regex extraction (e.g., \"2 years\")\n",
        "    age_extracted = df['Age upon Intake'].astype(str).str.extract(r'(?P<num>\\d+)\\s*(?P<unit>\\w+)', expand=True)\n",
        "    def convert_age(row):\n",
        "        try:\n",
        "            num = int(row['num'])\n",
        "        except:\n",
        "            return np.nan\n",
        "        unit = row['unit'].lower() if pd.notna(row['unit']) else ''\n",
        "        if 'year' in unit:\n",
        "            return num * 365\n",
        "        elif 'month' in unit:\n",
        "            return num * 30\n",
        "        elif 'week' in unit:\n",
        "            return num * 7\n",
        "        elif 'day' in unit:\n",
        "            return num\n",
        "        else:\n",
        "            return np.nan\n",
        "    df['Age in Days'] = age_extracted.apply(convert_age, axis=1)\n",
        "\n",
        "    if 'Date of Birth' in df.columns:\n",
        "        df['Date of Birth'] = pd.to_datetime(df.get('Date of Birth'), errors='coerce')\n",
        "        df['Age from DOB'] = (df['Intake Time'] - df['Date of Birth']).dt.days\n",
        "        df['Age in Days'] = df['Age in Days'].fillna(df['Age from DOB'])\n",
        "\n",
        "    # Feature engineering for categorical/text features\n",
        "    df['Is Mix'] = df.get('Breed', '').str.contains('Mix', case=False, na=False).astype(int)\n",
        "    df['Primary Color'] = df.get('Color', '').str.split('/').str[0]\n",
        "    df['Secondary Color'] = df.get('Color', '').str.split('/').str[1].fillna('None')\n",
        "    df['Color Count'] = df.get('Color', '').str.count('/') + 1\n",
        "    df['Sex'] = df.get('Sex upon Intake', '').str.extract(r'([A-Za-z]+)', expand=False)\n",
        "    df['Neutered'] = df.get('Sex upon Intake', '').str.contains('Neutered|Spayed', case=False, na=False).astype(int)\n",
        "    df['Has Name'] = df.get('Name').notna().astype(int) if 'Name' in df.columns else 0\n",
        "    df['In Austin'] = df.get('Found Location', '').str.contains('Austin', case=False, na=False).astype(int)\n",
        "\n",
        "    for col in ['Intake Condition', 'Animal Type']:\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts()\n",
        "            rare = counts[counts < 5].index\n",
        "            df[col] = df[col].replace(rare, 'Other')\n",
        "\n",
        "    outcome = df['Outcome Type'] if is_train and 'Outcome Type' in df.columns else None\n",
        "\n",
        "    features = [\n",
        "        'Intake Type', 'Intake Condition', 'Animal Type', 'Sex', 'Neutered',\n",
        "        'Age in Days', 'Is Mix', 'Has Name', 'In Austin',\n",
        "        'Intake Year', 'Intake Month', 'Intake Day', 'Intake Weekday', 'Intake Hour',\n",
        "        'Primary Color', 'Secondary Color', 'Color Count'\n",
        "    ]\n",
        "    if is_train and 'Length of Stay' in df.columns:\n",
        "        features.append('Length of Stay')\n",
        "\n",
        "    df = df[[col for col in features if col in df.columns]]\n",
        "    df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return (df, outcome) if is_train else df\n",
        "\n",
        "preprocess_data = memory.cache(preprocess_data)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Load Data and Preprocess\n",
        "# ---------------------------\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "X, y = preprocess_data(train_df, is_train=True)\n",
        "X_test = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "# Align columns (if \"Length of Stay\" is not present in test)\n",
        "if 'Length of Stay' in X.columns and 'Length of Stay' not in X_test.columns:\n",
        "    X = X.drop(columns=['Length of Stay'])\n",
        "X_test = X_test.loc[:, X.columns]\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Remove rare classes (if any class appears fewer than 2 times)\n",
        "class_counts = pd.Series(y_encoded).value_counts()\n",
        "rare_classes = class_counts[class_counts < 2].index\n",
        "if len(rare_classes) > 0:\n",
        "    indices_to_keep = [i for i, label in enumerate(y_encoded) if label not in rare_classes]\n",
        "    X = X.iloc[indices_to_keep]\n",
        "    y_encoded = np.array(y_encoded)[indices_to_keep]\n",
        "\n",
        "# Split into training and validation sets with stratification\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Drop any remaining all-NaN columns (safety check)\n",
        "X_train.dropna(axis=1, how='all', inplace=True)\n",
        "X_val.dropna(axis=1, how='all', inplace=True)\n",
        "X_test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Build Preprocessing Pipeline (One-Hot Encoding for Categoricals)\n",
        "# ---------------------------\n",
        "num_feats, cat_feats = numeric_features, categorical_features  # From above\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "preproc_pipeline = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, num_feats),\n",
        "    ('cat', categorical_pipeline, cat_feats)\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Build AdvancedPreprocessor Transformer (Flattened)\n",
        "# ---------------------------\n",
        "class AdvancedPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_components=50, n_clusters=5):\n",
        "        self.n_components = n_components\n",
        "        self.n_clusters = n_clusters\n",
        "        self.preprocessor = preproc_pipeline  # Use our preprocessor pipeline\n",
        "        self.pca = PCA(n_components=self.n_components, random_state=42)\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit preprocessor\n",
        "        X_pre = self.preprocessor.fit_transform(X)\n",
        "        self.pca.fit(X_pre)\n",
        "        X_pca = self.pca.transform(X_pre)\n",
        "        self.kmeans.fit(X_pca)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_pre = self.preprocessor.transform(X)\n",
        "        X_pca = self.pca.transform(X_pre)\n",
        "        clusters = self.kmeans.predict(X_pca)\n",
        "        cluster_dummies = pd.get_dummies(clusters, prefix=\"cluster\").values\n",
        "        return np.hstack([X_pre, cluster_dummies])\n",
        "\n",
        "advanced_preproc = AdvancedPreprocessor(n_components=50, n_clusters=5)\n",
        "\n",
        "# ---------------------------\n",
        "# 8. Build Stacking Classifier as the Ensemble\n",
        "# ---------------------------\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "base_estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42, n_jobs=-1)),\n",
        "    ('xgb', XGBClassifier(n_estimators=150, max_depth=4, learning_rate=0.1,\n",
        "                          use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,50), alpha=0.0001, activation='relu',\n",
        "                          max_iter=300, random_state=42))\n",
        "]\n",
        "meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 9. Build the Combined Pipeline using imblearn's Pipeline (Flattened)\n",
        "# ---------------------------\n",
        "combined_pipeline = ImbPipeline(steps=[\n",
        "    ('advanced_preproc', advanced_preproc),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', stacking_clf)\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 10. Hyperparameter Tuning for the Advanced Pipeline\n",
        "# ---------------------------\n",
        "param_grid = {\n",
        "    'advanced_preproc__n_components': [30, 50, 70],\n",
        "    'advanced_preproc__n_clusters': [3, 5, 7],\n",
        "    'classifier__rf__n_estimators': [100, 150, 200],\n",
        "    'classifier__rf__max_depth': [12, 15],\n",
        "    'classifier__xgb__n_estimators': [100, 150, 200],\n",
        "    'classifier__xgb__max_depth': [4, 6],\n",
        "    'classifier__xgb__learning_rate': [0.1, 0.01],\n",
        "    'classifier__mlp__hidden_layer_sizes': [(100,), (100,50), (150,100)],\n",
        "    'classifier__mlp__alpha': [0.0001, 0.001],\n",
        "    'classifier__mlp__activation': ['relu', 'tanh']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(combined_pipeline, param_distributions=param_grid,\n",
        "                                   n_iter=10, cv=3, scoring='accuracy',\n",
        "                                   n_jobs=-1, verbose=1, random_state=42)\n",
        "print(\"Tuning the Advanced Pipeline (PCA + K-Means + SMOTE + Stacking)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------\n",
        "# 11. Evaluate on the Validation Set\n",
        "# ---------------------------\n",
        "best_model = random_search.best_estimator_\n",
        "val_pred = best_model.predict(X_val)\n",
        "val_proba = best_model.predict_proba(X_val)\n",
        "print(\"\\n--- Validation Results ---\")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
        "print(f\"Validation Log Loss: {log_loss(y_val, val_proba):.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 12. Retrain Best Model on Full Data and Predict on Test Set\n",
        "# ---------------------------\n",
        "best_model.fit(X, y_encoded)\n",
        "test_pred = best_model.predict(X_test)\n",
        "\n",
        "# ---------------------------\n",
        "# 13. Save Final Submission\n",
        "# ---------------------------\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'Outcome Type': le.inverse_transform(test_pred)\n",
        "})\n",
        "submission.to_csv('4g.csv', index=False)\n",
        "print(\"\\nðŸ“„ Submission file saved as '4.csv'\")\n",
        "print(submission.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}